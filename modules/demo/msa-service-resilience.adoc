## Service Resilience and Fault Tolerance

[format="csv",cols="2"]
|======
*Length*,TBD
*Difficulty*,Medium
*Slides*,https://docs.google.com/presentation/d/1bt4k9yB0wDOj0d5WzDCWqftPxIizQ7f5S15LysEGFyQ/edit#slide=id.g1b95a791a8_0_24
*Video*,TBD
*Simulation (Mac)*,TBD
|======

### Explain the Demo Concepts

* Explain the use-case: services do fail. Sometimes there are temporary
shortages in the infrastructure and sometimes its a severe application
error that leads to failure. Its not a question if that happens, its a
matter of when. Applications and infrastructure needs to design for
service failure and limit the scope of failure in order to keep
providing the service even though some services might not function
properly. As an example, if the Inventory service fails that could lead
to the entire CoolStore web failing and preventing customers from
placing orders. When build for failure, Inventory failure would be
limited to the inventory status and would allow users to keep placing
orders without knowing about the inventory.

### Demonstrate Container Lifecycle Management

* Explain OpenShift container health management using
https://docs.google.com/presentation/d/1bt4k9yB0wDOj0d5WzDCWqftPxIizQ7f5S15LysEGFyQ/edit#slide=id.g1b95a791a8_0_24[the slides]
* In the OpenShift web console, go to *CoolStore PROD* project
* Scroll down to find the *Inventory* service group
* Explain that the blue circle shows the number containers that back *CoolStore GW* service
* Click on the up arrow to scale the service to two containers
* Explain that in a few seconds, a new identical container is deployed
and added automatically to the load-balancer. If one container fails,
there will be another container that can service requests

image::/images/demo/msa-resilience-scaled.png[Scaled Up,width=540,align=center]

* Click on the blue circle
* Explain that the list of pods backing the *Coolstore GW* is displayed
* Click on one of the containers with a name similar to *coolstore-gw-n-nnnnn*
* Explain that you can see the pod details such as
** The container image that is deployed
** The host that container is deployed on
** Persistent storage attached to the container
** Memory and CPU configurations
** Health and number of times its restarted +
* Click on *Actions* button and then *Delete* to delete this pod

image::/images/demo/msa-resilience-delete-pod.png[Delete Pod,width=920,align=center]

* Click on *Overview* in the left sidebar menu
* Explain that OpenShift immediately realizes that number of pods
backing the *CoolStore GW* service is reduced to 1 while it was declared
to have 2 pods backing this service for high-availability. OpenShift
restarts the removed pod in order to bring the number of pods back to 2 pods.

image::/images/demo/msa-resilience-auto-healing.png[Auto Healing,width=540,align=center]

* Explain that OpenShift allows distinguishing between failures that
might resolve with a restart and more severe issues that need required
further investigation. In latter cases, OpenShift is able to remove
those pods from the load-balancer and send user to the healthy
containers



### Demonstrate Service Resilience and Preventing Cascading Failures [Work-In-Progress]

* Explain service resilience using
https://docs.google.com/presentation/d/1bt4k9yB0wDOj0d5WzDCWqftPxIizQ7f5S15LysEGFyQ/edit#slide=id.g1b95a791a8_0_24[the slides]
* Explain Netflix OSS using
https://docs.google.com/presentation/d/1bt4k9yB0wDOj0d5WzDCWqftPxIizQ7f5S15LysEGFyQ/edit#slide=id.g1b95a791a8_0_24[the slides]
* Click twice on the down arrow on *Coolstore GW* pods blue circle. Click
on *Scale Down* button when it asks for confirmation in order to scale to 0
* Explain that the Coo
* - Service Resilience: keep functioning when services fail
*   - Explain the use case: Services fail. Sometimes die to hick up
and just need to get restarted and sometimes severely. We need to build
for both
*   - Explain OpenShift health checks using slides
*   - Explain cascading failures and circuit breaker using slides
*   - Go to web ui
*   - Explain the inventory microservice and point out the inventory
for each product
*   - Go to OpenShift console
*   - Point out the inventory container
*   - Delete the inventory container
*   - Explain that platform detects it and and starts the container
*   - Scale the container to zero
*   - Go to web ui and refresh the page
*   - Explain that effect of inventory service being down and reduce functionality instead of cascading failure
